================================================================================
搭 BACKLOG DE ATIVIDADES - SISTEMA DE MONITORAMENTO (SENIOR REVIEW V2)
================================================================================

Fala Jﾃｺnior, analisei seus pontos. Para melhorar de verdade a performance de um 
ecossistema Banco + App, nﾃ｣o basta sﾃｳ botar botﾃ｣o na tela. Precisamos garantir 
que o MySQL trabalhe menos e que o Python seja mais eficiente no trﾃ｡fego de dados.

Abaixo estﾃ｣o as tarefas focadas em PERFORMANCE REAL.

--------------------------------------------------------------------------------
噫 TAREFA 1: PAGINAﾃﾃグ NO FRONT, BACK E SQL (USER EXPERIENCE)
--------------------------------------------------------------------------------
PROBLEMA: Carregar 5000 livros explode a memﾃｳria do navegador e mata o banco.
SOLUﾃﾃグ: Paginaﾃｧﾃ｣o real (LIMIT/OFFSET).

ONDE MEXER:
1. `relatorio/app/database.py`: Altere `carregar_livros_do_banco(limit=10, offset=0)`.
2. `relatorio/app/routes/web_routes.py`: Receba `page` da URL e calcule o offset.
3. `relatorio/templates/index.html`: Botﾃ｣o "Carregar Mais" via JavaScript/Fetch 
   para nﾃ｣o dar refresh na pﾃ｡gina.

--------------------------------------------------------------------------------
笞｡ TAREFA 2: INDEXAﾃﾃグ E VIEW NO BANCO DE DATAS (DB PERFORMANCE)
--------------------------------------------------------------------------------
PROBLEMA: O comando GROUP BY e JOIN em tabelas grandes ﾃｩ lento porque o banco 
faz "Full Table Scan".

Aﾃﾃ髭S:
1. Banco de Dados: Criar INDEX nas chaves estrangeiras (`CODG_AUTOR_FK`, 
   `CODG_LIVRO_FK`) e na coluna de busca (`TITULO`).
2. Banco de Dados: Criar uma VIEW chamada `vw_resumo_livros`. 
   Mover toda a lﾃｳgica complexa do SELECT (JOINs + SUM + GROUP BY) para essa View.
3. `relatorio/app/database.py`: Simplificar a query para `SELECT * FROM vw_resumo_livros`.

--------------------------------------------------------------------------------
逃 TAREFA 3: BULK INSERT NA IMPORTAﾃﾃグ (IMPORT PERFORMANCE)
--------------------------------------------------------------------------------
PROBLEMA: Rodar um `execute()` para cada linha do CSV ﾃｩ um pecado capital. 
Cada comando ﾃｩ uma viagem de ida e volta ao banco (network roundtrip).

Aﾃﾃグ:
1. `relatorio/app/routes/web_routes.py`: Em vez de chamar a funﾃｧﾃ｣o para cada linha, 
   acumule os dados em uma lista de 100 em 100.
2. `relatorio/app/database.py`: Criar uma funﾃｧﾃ｣o que use `cursor.executemany()`. 
   Isso permite que o MySQL processe 100 linhas em uma ﾃｺnica chamada. 
   A velocidade de importaﾃｧﾃ｣o vai aumentar em atﾃｩ 10x.

--------------------------------------------------------------------------------
迫 TAREFA 4: CONNECTION POOLING (RESOURCES SAVING)
--------------------------------------------------------------------------------
PROBLEMA: Abrir e fechar conexﾃ｣o (`get_conn()`) em toda requisiﾃｧﾃ｣o gasta 
muito processamento do servidor.

Aﾃﾃグ:
1. `relatorio/app/database.py`: Configurar o `mysql.connector.pooling`. 
2. Criar um Pool de conexﾃｵes global no inﾃｭcio do arquivo e fazer o `get_conn()` 
   apenas retirar uma conexﾃ｣o "quente" do pool.

--------------------------------------------------------------------------------
識 CRITﾃ嘘IOS DE ACEITE:
- Importaﾃｧﾃ｣o de um CSV de 10k linhas em menos de 5 segundos.
- Busca por tﾃｭtulo retornando resultado instantﾃ｢neo.
- Uso de memﾃｳria do servidor estﾃ｡vel.

Vamos focar no que dﾃ｡ resultado! Qualquer dﾃｺvida na implementaﾃｧﾃ｣o do Pool 
ou do Executemany, dﾃ｡ um grito. 噫
================================================================================
